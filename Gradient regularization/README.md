The gradient regularization is a regularization method that adds the loss gradient norm penalty to the loss:
$$\widetilde{L}(\theta):=L(\theta)+\lamabda\lVert \nabla L(\theta)\rVert^2$$
