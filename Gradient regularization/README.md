The gradient regularization is a regularization method that adds the loss gradient norm penalty to the loss:

$$\widetilde{L}(\theta):=L(\theta)+\lambda\lVert \nabla L(\theta)\rVert^2$$

$\sqrt{3x-1}+(1+x)^2$
