The gradient regularization is a regularization method that adds the loss gradient norm penalty to the loss:

$\widetilde{L}(\theta):=L(\theta)+\lambda\lVert \nabla L(\theta)\rVert^2$

When $a \ne 0$, there are two solutions to $(ax^2 + bx + c = 0)$ and they are
$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$

